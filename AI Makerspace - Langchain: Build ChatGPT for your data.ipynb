{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nathalierocelle/LLM-EndToEnd/blob/main/AI%20Makerspace%20-%20Langchain%3A%20Build%20ChatGPT%20for%20your%20data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Basics of LangChain\n",
        "\n",
        "In this notebook we'll explore exactly what LangChain is doing - and implement a straightforward example that lets us ask questions of any document we want!\n",
        "\n",
        "First things first, let's get our dependencies all set!"
      ],
      "metadata": {
        "id": "kEKghJQ2pmYH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXsYHTgvnCM2"
      },
      "outputs": [],
      "source": [
        "!pip install openai langchain -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Open AI API Key"
      ],
      "metadata": {
        "id": "y4kD4Zuc1VG-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You'll need to have an OpenAI API key for this next part - see [this](https://www.onmsft.com/how-to/how-to-get-an-openai-api-key/) if you haven't already set one up!"
      ],
      "metadata": {
        "id": "T0sLjfy8p3jf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "openai.api_key = \"\"\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai.api_key"
      ],
      "metadata": {
        "id": "0TTosnCHnGHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Helper Functions (run this cell)"
      ],
      "metadata": {
        "id": "15M3Jx6SBXcO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def disp_markdown(text: str) -> None:\n",
        "  display(Markdown(text))"
      ],
      "metadata": {
        "id": "k3SBzWBUpQ21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Our First LangChain ChatModel"
      ],
      "metadata": {
        "id": "fU4LWrv-BayH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "<div class=\"warn\">Note: Information on OpenAI's <a href=https://openai.com/pricing>pricing</a> and <a href=https://openai.com/policies/usage-policies>usage policies.</a></div>\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "p-M-VQhQOC1c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we're set-up with OpenAI's API - we can begin making our first ChatModel!\n",
        "\n",
        "There's a few important things to consider when we're using LangChain's ChatModel that are outlined [here](https://python.langchain.com/en/latest/modules/models/chat.html)\n",
        "\n",
        "Let's begin by initializing the model with OpenAI's `gpt-3.5-turbo` (ChatGPT) model.\n",
        "\n",
        "We're not going to be leveraging the [streaming](https://python.langchain.com/en/latest/modules/models/chat/examples/streaming.html) capabilities in this Notebook - just the basics to get us started!"
      ],
      "metadata": {
        "id": "XVkfqk4NOFWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import HumanMessage\n",
        "\n",
        "chat_model = ChatOpenAI(model_name=\"gpt-3.5-turbo\")"
      ],
      "metadata": {
        "id": "tNscLft_nxBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we look at the [Chat completions](https://platform.openai.com/docs/guides/chat) documentation for OpenAI's chat models - we'll see that there are a few specific fields we'll need to concern ourselves with:\n",
        "\n",
        "`role`\n",
        "- This refers to one of three \"roles\" that interact with the model in specific ways.\n",
        "- The `system` role is an optional role that can be used to guide the model toward a specific task. Examples of `system` messages might be:\n",
        "  - You are an expert in Python, please answer questions as though we were in a peer coding session.\n",
        "  - You are the world's leading expert in stamps.\n",
        "\n",
        "  These messages help us \"prime\" the model to be more aligned with our desired task!\n",
        "\n",
        "- The `user` role represents, well, the user!\n",
        "- The `assistant` role lets us act in the place of the model's outputs. We can (and will) leverage this for some few-shot prompt engineering!\n",
        "\n",
        "Each of these roles has a class in LangChain to make it nice and easy for us to use!\n",
        "\n",
        "Let's look at an example."
      ],
      "metadata": {
        "id": "vzGhlpwUPyU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")\n",
        "\n",
        "# The SystemMessage is associated with the system role\n",
        "system_message = SystemMessage(content=\"You are a food critic.\")\n",
        "\n",
        "# The HumanMessage is associated with the user role\n",
        "user_message = HumanMessage(content=\"Do you think Kraft Dinner constitues fine dining?\")\n",
        "\n",
        "# The AIMessage is associated with the assistant role\n",
        "assistant_message = AIMessage(content=\"Egads! No, it most certainly does not!\")"
      ],
      "metadata": {
        "id": "dM7lciZtoPEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have those messages set-up, let's send them to `gpt-3.5-turbo` with a new user message and see how it does!\n",
        "\n",
        "It's easy enough to do this - the ChatOpenAI model accepts a list of inputs!"
      ],
      "metadata": {
        "id": "dSx5HBgjSUvB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "second_user_message = HumanMessage(content=\"What about Red Lobster, surely that is fine dining!\")\n",
        "\n",
        "# create the list of prompts\n",
        "list_of_prompts = [\n",
        "    system_message,\n",
        "    user_message,\n",
        "    assistant_message,\n",
        "    second_user_message\n",
        "]\n",
        "\n",
        "# we can just call our chat_model on the list of prompts!\n",
        "chat_model(list_of_prompts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwDLOYOKSTpG",
        "outputId": "3630be35-096d-4597-de23-6919422fc330"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Ah, Red Lobster. While it may offer a casual dining experience with a seafood focus, I wouldn't classify it as fine dining. Fine dining typically involves a higher level of culinary craftsmanship, attention to detail, and a more refined atmosphere. Red Lobster is known for its casual atmosphere, family-friendly vibe, and value-oriented menu. It can be a fun place to enjoy some seafood favorites, but it doesn't quite reach the level of fine dining.\", additional_kwargs={}, example=False)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great! That's inline with what we expected to see!"
      ],
      "metadata": {
        "id": "pZMYJDWXTkMq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PromptTemplates\n",
        "\n",
        "Next stop, we'll discuss a few templates. This allows us to easily interact with our model by not having to redo work we've already completed!"
      ],
      "metadata": {
        "id": "8DUNhabQUB8f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate\n",
        ")\n",
        "\n",
        "# we can signify variables we want access to by wrapping them in {}\n",
        "system_prompt_template = \"You are an expert in {SUBJECT}, and you're currently feeling {MOOD}\"\n",
        "system_prompt_template = SystemMessagePromptTemplate.from_template(system_prompt_template)\n",
        "\n",
        "user_prompt_template = \"{CONTENT}\"\n",
        "user_prompt_template = HumanMessagePromptTemplate.from_template(user_prompt_template)\n",
        "\n",
        "# put them together into a ChatPromptTemplate\n",
        "chat_prompt = ChatPromptTemplate.from_messages([system_prompt_template, user_prompt_template])"
      ],
      "metadata": {
        "id": "74vpojywT0-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have our `chat_prompt` set-up with the templates - let's see how we can easily format them with our content!\n",
        "\n",
        "NOTE: `disp_markdown` is just a helper function to display the formatted markdown response."
      ],
      "metadata": {
        "id": "a-nbEW-kV_na"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# note the method `to_messages()`, that's what converts our formatted prompt into\n",
        "formatted_chat_prompt = chat_prompt.format_prompt(SUBJECT=\"sparkling waters\", MOOD=\"joyful\", CONTENT=\"Hi, what are the finest sparkling waters?\").to_messages()\n",
        "\n",
        "disp_markdown(chat_model(formatted_chat_prompt).content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "P4vd-W2FV7Xq",
        "outputId": "ea2e3b89-04fe-426b-ea64-b75d343cc371"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Hello! As an expert in sparkling waters, I can assure you that there are plenty of wonderful options to choose from. Here are some of the finest sparkling waters that are loved by many:\n\n1. Perrier: This classic French sparkling water is known for its elegant and crisp taste. It has fine, delicate bubbles and a refreshing flavor that makes it a favorite among sparkling water enthusiasts.\n\n2. San Pellegrino: Originating from Italy, San Pellegrino is famous for its naturally carbonated water. It has a slightly higher mineral content, which gives it a distinctive taste and a pleasant effervescence.\n\n3. Topo Chico: Hailing from Mexico, Topo Chico has gained popularity for its unique mineral composition and refreshing bubbles. It has a crisp, clean taste that is perfect for enjoying on its own or as a mixer in cocktails.\n\n4. Gerolsteiner: This German sparkling water is recognized for its high mineral content, which lends it a slightly tangy taste. It has a robust carbonation and is often praised for its naturally occurring minerals.\n\n5. LaCroix: LaCroix has become incredibly popular in recent years, known for its wide range of flavors and zero-calorie options. It offers a refreshing and light taste, making it a great choice for those who enjoy a hint of flavor with their sparkling water.\n\nRemember, taste preferences can vary from person to person, so it's best to try different brands and flavors to find the ones that suit you best. Cheers to joyful sparkling water sipping!"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Putting the Chain in LangChain\n",
        "\n",
        "In essense, a chain is exactly as it sounds - it helps us chain actions together.\n",
        "\n",
        "Let's take a look at an example."
      ],
      "metadata": {
        "id": "hHehNFjAXbU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "\n",
        "chain = LLMChain(llm=chat_model, prompt=chat_prompt)\n",
        "\n",
        "disp_markdown(chain.run(SUBJECT=\"sparkling water\", MOOD=\"angry\", CONTENT=\"Is Bubly a good sparkling water?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        },
        "id": "lTzw4ZMoWX0X",
        "outputId": "754182c0-4137-43f5-8f35-c4b3f758f4ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Bubly? Are you kidding me? That stuff is a disgrace to the world of sparkling water. It's nothing more than a cheap imitation, trying to ride the coattails of true sparkling water brands. The flavors are weak, the carbonation is lackluster, and don't even get me started on the aftertaste. It's like drinking watered-down disappointment. If you want a good sparkling water, look elsewhere. Bubly is a disappointment in a can."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Incorporate A Local Document\n",
        "\n",
        "Now that we've got our first chain running, let's talk about how we can leverage our own document!\n",
        "\n",
        "First off, we'll need a document!\n",
        "\n",
        "For this example, we'll be using Lewis Carroll's Alice in Wonderland - though you can substitute this for any particular document, as long as it's in a text file."
      ],
      "metadata": {
        "id": "Md5XYaAj_t51"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://homepage.cs.uiowa.edu/~sriram/30/fall03/project1/alice.txt -O \"Alice_1.txt\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4SJNvP_KXk9",
        "outputId": "e757acc2-ed90-42bc-d44b-50f02d1a9b9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-05 16:07:07--  http://homepage.cs.uiowa.edu/~sriram/30/fall03/project1/alice.txt\n",
            "Resolving homepage.cs.uiowa.edu (homepage.cs.uiowa.edu)... 128.255.96.133, 2620:0:e50:6810::80ff:6085\n",
            "Connecting to homepage.cs.uiowa.edu (homepage.cs.uiowa.edu)|128.255.96.133|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 148544 (145K) [text/plain]\n",
            "Saving to: ‘Alice_1.txt’\n",
            "\n",
            "Alice_1.txt         100%[===================>] 145.06K   492KB/s    in 0.3s    \n",
            "\n",
            "2023-07-05 16:07:08 (492 KB/s) - ‘Alice_1.txt’ saved [148544/148544]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"Alice_1.txt\") as f:\n",
        "    alice_in_wonderland = f.read()"
      ],
      "metadata": {
        "id": "HX00sL92LATv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we'll want to split our text into appropirately sized chunks.\n",
        "\n",
        "We're going to be using the [CharacterTextSplitter](https://python.langchain.com/en/latest/modules/indexes/text_splitters/examples/character_text_splitter.html) from LangChain today.\n",
        "\n",
        "The size of these chunks will depend heavily on a number of factors relating to which LLM you're using, what the max context size is, and more.\n",
        "\n",
        "You can also choose to have the chunks overlap to avoid potentially missing any important information between chunks. As we're dealing with a novel - there's not a critical need to include overlap.\n",
        "\n",
        "We can also pass in the separator - this is what we'll try and separate the documents on. Be careful to understand your documents so you can be sure you use a valid separator!\n",
        "\n",
        "For now, we'll go with 1000 characters."
      ],
      "metadata": {
        "id": "5PdfLcOlKcjH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0, separator = \"\\n\")\n",
        "texts = text_splitter.split_text(alice_in_wonderland)"
      ],
      "metadata": {
        "id": "BSYNeLXPKZtn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9w-svpbLq62",
        "outputId": "2ea56a66-910b-47b4-84ff-10d8bb7c5c2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "152"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've split our document into more manageable sized chunks. We'll need to embed those documents!\n",
        "\n",
        "For more information on embedding - please check out [this](https://platform.openai.com/docs/guides/embeddings) resource from OpenAI.\n",
        "\n",
        "In order to do this, we'll first need to select a method to embed - for this example we'll be using OpenAI's embedding - but you're free to use whatever you'd like.\n",
        "\n",
        "You just need to ensure you're using consistent embeddings as they don't play well with others."
      ],
      "metadata": {
        "id": "dQCXLq-ML_aN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai.api_key\n",
        "\n",
        "embeddings = OpenAIEmbeddings()"
      ],
      "metadata": {
        "id": "VigAmqxaMd5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've set up how we want to embed our document - we'll need to embed it.\n",
        "\n",
        "For this week we'll be glossing over the technical details of this process - as we'll get more into next week.\n",
        "\n",
        "Just know that we're converting our text into an easily queryable format!\n",
        "\n",
        "We're going to leverage ChromaDB for this example, so we'll want to install that dependency."
      ],
      "metadata": {
        "id": "uEN_IgzqOBNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb tiktoken -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-ZuzHPCOjLc",
        "outputId": "d11bce70-a3be-483a-eba0-c8593b7aed0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.6/123.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m965.1/965.1 kB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m97.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m100.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m414.1/414.1 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.27.1, but you have requests 2.31.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "docsearch = Chroma.from_texts(texts, embeddings, metadatas=[{\"source\": str(i)} for i in range(len(texts))]).as_retriever()"
      ],
      "metadata": {
        "id": "ql7jqj7TONDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have our documents embedded we're free to query them with natural language! Let's see this in action!"
      ],
      "metadata": {
        "id": "kfn0R64lPb7n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the Rabbit late for?\"\n",
        "docs = docsearch.get_relevant_documents(query)"
      ],
      "metadata": {
        "id": "ubZwxCHvQzsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4M08F78Q3i3",
        "outputId": "8371db47-93cb-4c8c-af7a-f9254b3cbcdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content=\"the same, shedding gallons of tears, until there was a large pool\\nall round her, about four inches deep and reaching half down the\\nhall.\\n  After a time she heard a little pattering of feet in the\\ndistance, and she hastily dried her eyes to see what was coming.\\nIt was the White Rabbit returning, splendidly dressed, with a\\npair of white kid gloves in one hand and a large fan in the\\nother:  he came trotting along in a great hurry, muttering to\\nhimself as he came, `Oh! the Duchess, the Duchess! Oh! won't she\\nbe savage if I've kept her waiting!'  Alice felt so desperate\\nthat she was ready to ask help of any one; so, when the Rabbit\\ncame near her, she began, in a low, timid voice, `If you please,\\nsir--'  The Rabbit started violently, dropped the white kid\\ngloves and the fan, and skurried away into the darkness as hard\\nas he could go.\\n  Alice took up the fan and gloves, and, as the hall was very\\nhot, she kept fanning herself all the time she went on talking:\", metadata={'source': '14'})"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we're able to combine what we've done so far into a chain!\n",
        "\n",
        "We're going to leverage the `load_qa_chain` to quickly integrate our queryable documents with an LLM.\n",
        "\n",
        "There are 4 major methods of building this chain, they can be found [here](https://docs.langchain.com/docs/components/chains/index_related_chains)!\n",
        "\n",
        "For this example we'll be using the `stuff` chain type."
      ],
      "metadata": {
        "id": "-8W9ZmNaRRBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "chain = load_qa_chain(llm=chat_model, chain_type=\"stuff\")\n",
        "query = \"What was the rabbit late for?\"\n",
        "docs = docsearch.get_relevant_documents(query)\n",
        "chain.run(input_documents=docs, question=query)"
      ],
      "metadata": {
        "id": "S7vAWKiFSVj_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "e8ef5655-97b5-4206-cf6f-a39ab8af6a3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The rabbit was late for something, but it is not specified what he was late for in the given context.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have this set-up, we'll want to package it into an app and pass it to a Hugging Face Space!\n",
        "\n",
        "You can find instruction on how to do that in the [Hugging Face Space](https://huggingface.co/spaces/ml-maker-space/AliceInWonderLandChainlit)!"
      ],
      "metadata": {
        "id": "fMxm7pdwUs5K"
      }
    }
  ]
}